%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Instructions for IWCS 2021 Proceedings}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
This document contains the instructions for preparing a manuscript for the proceedings of ACL-IJCNLP 2021.
The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like.
These instructions should be used for both papers submitted for review and for final versions of accepted papers.
Authors are asked to conform to all the directions reported in this document.
\end{abstract}

\section{Introduction}
Today's Large Language Models (LLMs) can write production code \cite{jiang2024surveylargelanguagemodels}, translate fluently across 100+ languages \cite{zhu2024multilingualmachinetranslationlarge}, and remember conversational nuances for hours-long chat (cite), but somehow misplace a marble when Sally leaves the room. Understanding meaning, especially the meaning of mental state attributions, remains fundamentally problematic for LLMs. GPT-4's performance on simple false-belief tasks is 75-80\% (humans get 87\%) \cite{moghaddam2023boostingtheoryofmindperformancelarge, Kosinski_2024}, but it struggles with complex scenario tasks. LLMs' performance 30-60\% less, when tasks involve understanding why people lie, tracking multiple perspectives, or reading social cues, which is worse than humans' performance on the same tasks \cite{sap-etal-2022-neural, kim-etal-2023-fantom}. These failures reveal a fundamental limitation in semantic understanding.


Theory of Mind (ToM) allows humans to predict behavior by modeling others' mental states, even when their beliefs contradict reality \cite{Premack_Woodruff_1978}. It presents a core challenge in computational semantics to process intensional contexts where agents' beliefs diverge from reality. The sentence ``Sally believes the marble is in the basket'' creates an opaque context, where the truth of the embedded clause is evaluated relative to Sally's belief state, but not the actual world \cite{Montague1973-MONTPT-4}. This semantic complexity becomes evident in LLMs' failure patterns, where models that can solve standard false-belief tasks fail when containers become transparent or locations change \cite{ullman2023largelanguagemodelsfail}. These failures suggest LLMs lack compositional understanding of how mental state verbs like ``believe,'' ``know'' and ``think'' can create distinct modal contexts that block standard entailment relations \cite{Karttunen73}.

In this paper, we empirically analyze six LLMs on Theory of Mind tasks to understand their semantic failures. We quantify features at multiple linguistic levels, including information structure (idea density), discourse relations (RST parsing), lexical patterns (mental state verb distribution), and distributional semantics (semantic similarity), to address three key research questions: (1) To what extent do these features predict LLM success on mental state reasoning? (2) Do different models reveal distinct patterns in processing intensional contexts? (3) What systematic failures emerge across model architectures? We find that the complexity metrics show surprisingly weak correlations with performance on the ToM tasks, revealing that current LLMs neither leverage linguistic complexity cues nor employ compositional semantic processing, suggesting reliance on task-specific patterns or spurious correlations.

%Classic tests like the Sally-Anne paradigm \cite{baron-coonhen_does_1985} evaluate false-belief understanding through well-structured narratives. LLMs often get the test right, but fail when researchers make small changes, like introducing transparent boxes instead of opaque ones, or creating different hiding spots. Since humans do not have these trouble with changes, it shows LLMs are just recognizing familiar patterns, but not truly understanding mental states 

%Analyzing the semantic composition of ToM tasks reveals why LLMs fail where humans succeed. Computational semantics reveals that ToM scenarios involve tracking meaning across three layers: narrative contexts that constrain interpretations, mental state verbs that mark perspective boundaries, and answer options that demand different inferential distances from the given text. The semantic complexity involved may explain the systematic ways models fail.

\section{Related Work}
%\subsection{Theory of Mind Evaluation in Large Language Models}
Modern ToM testing goes much further than classic false-belief scenarios. ToMBench dataset \cite{chen-etal-2024-tombench} measures 31 aspects of social cognition using 8 different tasks and 6 categories, showing persistent model weaknesses. The benchmark uses two languages and multiple-choice questions to prevent memorization and allow automatic scoring. EPITOME \cite{jones-etal-2024-comparing-humans} applies psychology research methods to categorize ToM errors into seven types. Models fail more frequent on pragmatic reasoning and social inference tasks, while performing better on basic belief questions. Researchers now intensionally use difficult test cases to uncover more limitations. ExploreToM generates difficult story structures using A* search algorithms to test how well models handle complex compositions. The Two Word Test study \cite{riccardi2023wordtestsemanticbenchmark} finds that models fail at simple noun-noun combinations. These new evaluation methods show that models often succeed by memorizing common patterns rather than actually tracking beliefs.

%\subsection{Compositional Processing Limitations}
%Broader evidence reveals fundamental compositional limitations in neural language models. \citeauthor{dankers-etal-2022-paradox} stated that transformer models lack consistent compositionality, means they unexpectedly succeed on some complex cases, but fail on simpler ones, showing no clear rules for how they process language. \citeauthor {du2023shortcutlearninglargelanguage} found three ways LLMs avoid proper reasoning: overlap bias (matching similar features), position bias (using token location), and style bias (recognizing text patterns without understanding meaning). Belief reasoning tasks reveal these same problems.

\section{Methodology}

\subsection{Data}
We used the ToMBench dataset \cite{chen-etal-2024-tombench}, a benchmark designed to evaluate Theory of Mind capabilities in LLMs. We focused exclusively on the English version of the dataset. ToMBench consists of 31 distinct aspects of social cognition organized into 6 categories: beliefs, emotions, intentions, knowledge, non-literal communication, and desire. The dataset uses multiple-choice question (A, B, C, D) with underlying text answer formulation to facilitate automatic scoring and reduce ambiguity during evaluation. 
\subsection{Models}

\subsection{Experimental Setup}


\section{Results}

\begin{table*}[!htbp]
\centering
\caption{Model Performance by Theory of Mind Ability (\%)}
\label{tab:model_performance}
\begin{tabular}{l|cccccc|c}
\toprule
\textbf{Model} & \textbf{Emotion} & \textbf{Desire} & \textbf{Intention} & \textbf{Knowledge} & \textbf{Belief} & \textbf{NL Comm.} & \textbf{AVG} \\
\midrule
Human & 86.4 & 78.2 & 90.4 & 82.2 & 89.3 & 89.0 & 85.9 \\
\midrule
Llama 70B & 75.0 & 61.1 & 81.2 & 48.3 & 85.4 & 78.9 & 71.6 \\
Qwen 32B & 70.2 & 59.4 & 70.0 & 36.9 & 78.7 & 70.9 & 64.4 \\
OLMo 13B & 67.4 & 50.0 & 66.2 & 37.9 & 62.5 & 67.9 & 58.6 \\
Mistral 7B & 58.3 & 51.1 & 55.0 & 26.6 & 52.3 & 63.1 & 51.1 \\
Phi-3 Mini & 60.7 & 52.2 & 59.4 & 33.4 & 60.5 & 63.8 & 55.0 \\
InternLM 1.8B & 51.2 & 45.0 & 50.6 & 33.4 & 47.3 & 65.4 & 48.8 \\
\bottomrule
\end{tabular}
\end{table*}


\begin{table}[htbp]
\centering
\caption{Linguistic Metrics by Theory of Mind Ability}
\label{tab:linguistic_metrics}
\begin{tabular}{l|cccccc|c}
\toprule
\textbf{Metric} & \textbf{Emotion} & \textbf{Desire} & \textbf{Intention} & \textbf{Knowledge} & \textbf{Belief} & \textbf{NL Comm.} & \textbf{AVG} \\
\midrule
Idea Density & 0.434 & 0.407 & 0.423 & 0.387 & 0.336 & 0.430 & 0.403 \\
Num EDUs & 8.138 & 7.511 & 13.615 & 9.493 & 8.713 & 15.652 & 10.520 \\
RST Tree Depth & 4.274 & 4.128 & 5.997 & 5.148 & 4.016 & 6.992 & 5.092 \\
Rel Attribution & 0.950 & 1.228 & 1.526 & 1.486 & 0.422 & 3.531 & 1.524 \\
Rel Causal & 0.640 & 0.439 & 1.062 & 0.376 & 0.643 & 0.652 & 0.635 \\
Rel Explanation & 0.057 & 0.117 & 0.176 & 0.086 & 0.181 & 0.551 & 0.195 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{RQ1}


\subsection{RQ2}


\subsection{RQ3}


\section{Discussion}


\section{Conclusion}


%\section*{Acknowledgments}

%The acknowledgments should go immediately before the references. Do not number the acknowledgments section.
%\textbf{Do not include this section when submitting your paper for review.}

\bibliographystyle{acl_natbib}
\bibliography{anthology,acl2021}

%\appendix



\end{document}
