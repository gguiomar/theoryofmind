@misc{jiang2024surveylargelanguagemodels,
      title={A Survey on Large Language Models for Code Generation}, 
      author={Juyong Jiang and Fan Wang and Jiasi Shen and Sungju Kim and Sunghun Kim},
      year={2024},
      eprint={2406.00515},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.00515}, 
}

@misc{zhu2024multilingualmachinetranslationlarge,
      title={Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis}, 
      author={Wenhao Zhu and Hongyi Liu and Qingxiu Dong and Jingjing Xu and Shujian Huang and Lingpeng Kong and Jiajun Chen and Lei Li},
      year={2024},
      eprint={2304.04675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.04675}, 
}

@article{Kosinski_2024,
   title={Evaluating large language models in theory of mind tasks},
   volume={121},
   ISSN={1091-6490},
   url={http://dx.doi.org/10.1073/pnas.2405460121},
   DOI={10.1073/pnas.2405460121},
   number={45},
   journal={Proceedings of the National Academy of Sciences},
   publisher={Proceedings of the National Academy of Sciences},
   author={Kosinski, Michal},
   year={2024},
   month=oct }

@inproceedings{sap-etal-2022-neural,
    title = "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large {LM}s",
    author = "Sap, Maarten  and
      Le Bras, Ronan  and
      Fried, Daniel  and
      Choi, Yejin",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.248/",
    doi = "10.18653/v1/2022.emnlp-main.248",
    pages = "3762--3780",
    abstract = "Social intelligence and Theory of Mind (TOM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allows humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial.In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theorybased perspective. We show that one of today{'}s largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measure models' ability to understand intents and reactions of participants of social interactions, and ToMi (Le, Boureau, and Nickel, 2019), which measures whether models can infer mental states and realities of participants of situations.Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55{\%} and 60{\%} on SocialIQa and ToMi, respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms. Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind."
}

@article{Premack_Woodruff_1978, title={Does the chimpanzee have a theory of mind?}, volume={1}, DOI={10.1017/S0140525X00076512}, number={4}, journal={Behavioral and Brain Sciences}, author={Premack, David and Woodruff, Guy}, year={1978}, pages={515–526}} <div></div>

@article{baron-coonhen_does_1985,
	title = {Does the autistic child have a “theory of mind” ?},
	volume = {21},
	issn = {0010-0277},
	doi = {10.1016/0010-0277(85)90022-8},
	abstract = {We use a new model of metarepresentational development to predict a cognitive deficit which could explain a crucial component of the social impairment in childhood autism. One of the manifestations of a basic metarepresentational capacity is a ‘theory of mind’. We have reason to believe that autistic children lack such a ‘theory’. If this were so, then they would be unable to impute beliefs to others and to predict their behaviour. This hypothesis was tested using Wimmer and Perner's puppet play paradigm. Normal children and those with Down's syndrome were used as controls for a group of autistic children. Even though the mental age of the autistic children was higher than that of the controls, they alone failed to impute beliefs to others. Thus the dysfunction we have postulated and demonstrated is independent of mental retardation and specific to autism.
Les auteurs présentent un nouveau mod`éle de développement méta-cognitif pour prédire le déficit cognitif qui rendrait compte d'un composant essentiel du handicap social de l'enfant autiste. Une des manifestations d'une capacité de base méta-cognitive est une ‘theorie de l'esprit'. Nous avons des raisons de croire que cette théorie fait defaut chez l'enfant autiste. Celui-ci serait done incapable d'attribuer des croyances aux autres ou de prédire leur comportement. Cette hypothèse a été testée avec le paradigme de jeu des marionettes utilisé par Wimmer et Perner. Des enfants normaux et des enfants avec trisomie 21 ont servi de groupe contrôle. Bien que Page mental des enfants autistes ait été plus élevé que deux du groupe contrôle, seuls les enfants autistes Wont pu attribuer aux autres des croyances. Ainsi le dysfonctionnement prévu a pu être démontre, il s'avère indépendant du retard mental et spécifique a l'autiste.},
	number = {1},
	journal = {Cognition},
	author = {Baron-Cohen, Simon and Leslie, Alan M. and Frith, Uta},
	year = {1985},
	note = {Place: Amsterdam
Publisher: Elsevier B.V},
	keywords = {Adolescent, Autism, Autistic Disorder - psychology, Biological and medical sciences, Child, Child clinical studies, Communication disorders, Developmental psychology, Down Syndrome - psychology, Humans, Medical sciences, Mental age, Mental representation, Metarepresentation, Mind-blindness, Miscellaneous, Psychology, Psychology. Psychoanalysis. Psychiatry, Psychopathology. Psychiatry, Social Perception, Theory of mind, Theory-theory, Weak central coherence theory},
	pages = {37--46},
	annote = {ObjectType-Article-1},
}

@misc{ullman2023largelanguagemodelsfail,
      title={Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks}, 
      author={Tomer Ullman},
      year={2023},
      eprint={2302.08399},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2302.08399}, 
}

@misc{moghaddam2023boostingtheoryofmindperformancelarge,
      title={Boosting Theory-of-Mind Performance in Large Language Models via Prompting}, 
      author={Shima Rahimi Moghaddam and Christopher J. Honey},
      year={2023},
      eprint={2304.11490},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2304.11490}, 
}

@inproceedings{kim-etal-2023-fantom,
    title = "{FANT}o{M}: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
    author = "Kim, Hyunwoo  and
      Sclar, Melanie  and
      Zhou, Xuhui  and
      Bras, Ronan  and
      Kim, Gunhee  and
      Choi, Yejin  and
      Sap, Maarten",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.890/",
    doi = "10.18653/v1/2023.emnlp-main.890",
    pages = "14397--14413",
    abstract = "Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning."
}

@incollection{Montague1973-MONTPT-4,
	author = {Richard Montague},
	booktitle = {Approaches to Natural Language},
	editor = {Patrick Suppes and Julius Moravcsik and Jaakko Hintikka},
	pages = {221--242},
	publisher = {Dordrecht},
	title = {The Proper Treatment of Quantification in Ordinary English},
	year = {1973}
}

@article{Karttunen73,
	Author = {Lauri Karttunen},
	Date-Added = {2015-03-15 19:36:13 +0000},
	Date-Modified = {2016-09-23 16:21:00 +0000},
	Journal = {Linguistic Inquiry},
	Number = {2},
	Pages = {167-193},
	Title = {Presuppositions of Compound Sentences},
	Volume = {4},
	Year = {1973}}

@inproceedings{chen-etal-2024-tombench,
    title = "{T}o{MB}ench: Benchmarking Theory of Mind in Large Language Models",
    author = "Chen, Zhuang  and
      Wu, Jincenzi  and
      Zhou, Jinfeng  and
      Wen, Bosi  and
      Bi, Guanqun  and
      Jiang, Gongyao  and
      Cao, Yaru  and
      Hu, Mengting  and
      Lai, Yunghwei  and
      Xiong, Zexuan  and
      Huang, Minlie",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.847/",
    doi = "10.18653/v1/2024.acl-long.847",
    pages = "15959--15983",
    abstract = "Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10{\%} points, indicating that LLMs have not achieved a human-level theory of mind yet. Our aim with ToMBench is to enable an efficient and effective evaluation of LLMs' ToM capabilities, thereby facilitating the development of LLMs with inherent social intelligence."
}

@article{jones-etal-2024-comparing-humans,
    title = "Comparing Humans and Large Language Models on an Experimental Protocol Inventory for Theory of Mind Evaluation ({EPITOME})",
    author = "Jones, Cameron R.  and
      Trott, Sean  and
      Bergen, Benjamin",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "12",
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.tacl-1.45/",
    doi = "10.1162/tacl_a_00674",
    pages = "803--819",
    abstract = "We address a growing debate about the extent to which large language models (LLMs) produce behavior consistent with Theory of Mind (ToM) in humans. We present EPITOME: a battery of six experiments that tap diverse ToM capacities, including belief attribution, emotional inference, and pragmatic reasoning. We elicit a performance baseline from human participants for each task. We use the dataset to ask whether distributional linguistic information learned by LLMs is sufficient to explain ToM in humans. We compare performance of five LLMs to a baseline of responses from human comprehenders. Results are mixed. LLMs display considerable sensitivity to mental states and match human performance in several tasks. Yet, they commit systematic errors in others, especially those requiring pragmatic reasoning on the basis of mental state information. Such uneven performance indicates that human-level ToM may require resources beyond distributional information."
}

@misc{riccardi2023wordtestsemanticbenchmark,
      title={The Two Word Test: A Semantic Benchmark for Large Language Models}, 
      author={Nicholas Riccardi and Rutvik H. Desai},
      year={2023},
      eprint={2306.04610},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.04610}, 
}

@inproceedings{dankers-etal-2022-paradox,
    title = "The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study",
    author = "Dankers, Verna  and
      Bruni, Elia  and
      Hupkes, Dieuwke",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.286/",
    doi = "10.18653/v1/2022.acl-long.286",
    pages = "4154--4175",
    abstract = "Obtaining human-like performance in NLP is often argued to require compositional generalisation. Whether neural networks exhibit this ability is usually studied by training models on highly compositional synthetic data. However, compositionality in natural language is much more complex than the rigid, arithmetic-like version such data adheres to, and artificial compositionality tests thus do not allow us to determine how neural models deal with more realistic forms of compositionality. In this work, we re-instantiate three compositionality tests from the literature and reformulate them for neural machine translation (NMT).Our results highlight that: i) unfavourably, models trained on more data are more compositional; ii) models are sometimes less compositional than expected, but sometimes more, exemplifying that different levels of compositionality are required, and models are not always able to modulate between them correctly; iii) some of the non-compositional behaviours are mistakes, whereas others reflect the natural variation in data. Apart from an empirical study, our work is a call to action: we should rethink the evaluation of compositionality in neural networks and develop benchmarks using real data to evaluate compositionality on natural language, where composing meaning is not as straightforward as doing the math."
}

@misc{du2023shortcutlearninglargelanguage,
      title={Shortcut Learning of Large Language Models in Natural Language Understanding}, 
      author={Mengnan Du and Fengxiang He and Na Zou and Dacheng Tao and Xia Hu},
      year={2023},
      eprint={2208.11857},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2208.11857}, 
}